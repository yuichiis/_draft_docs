---
layout: document
title: "PHPでのアテンションを用いたニューラル機械翻訳"
upper_section: tutorials/tutorials
previous_section: tutorials/learn-add-numbers-with-rnn
---
このチュートリアルでは、PHPでリカレントニューラルネットワーク（RNN）とアテンションを使用して、フランス語から英語への変換モデルを構築します。

まず、自然言語を扱う際によく使用されるトークナイザーを使用して、フランス語のシーケンスと英語のシーケンスに文を変換します。
シーケンスからシーケンスへの変換を行う学習モデルをシーケンス・トゥ・シーケンス学習と呼びます。

この機械学習モデルでは、リカレントニューラルネットワーク（RNN）とアテンションを使用します。

このモデルでは、アテンションは入力のどの部分がモデルの注目を集めるかを示します。人間に似たモデルを構築することで、変換の精度が向上します。

![エンコーダとデコーダ](images/neural-machine-translation-attention.png)

事前準備
---------
開始する前に、Rindow Neural Networksをセットアップしてください。インストール手順は
[Rindow Neural Networksのインストール](/neuralnetworks/install.md)を参照してください。

PHPでもRNNが十分に高速に動作することを体験してください。
Windows環境を使用している場合は、Rindow CLBlast / OpenCLの使用をお勧めします。

[PHPでの基本的な画像分類](basic-image-classification.html)チュートリアルを完了した方、または同等の知識を持っている方に向けています。

データセット
-------------
http://www.manythings.org/anki/ から提供される様々な言語のデータを使用します。
このデータには、英語の文と他の言語に翻訳された文のペアが含まれています。
このチュートリアルでは、英語とフランス語のデータセットを使用します。

例えば、以下のようになります。
```
Let me do that.       Laissez moi faire ça.
```
このデータをモデルに入力できるデータに変換します。

まず、文を英語とフランス語の文に分割し、それぞれの文の先頭と末尾にマーカーを追加します。

```
English:   <start> Let me do that. <end>
French:    <start> Laissez moi faire ça. <end>
```

次に、これをトークナイザーでシーケンスに変換します。
トークナイザー内で以下の処理が行われます。

+ 文から特殊文字を削除します。
+ 単語に分割します。
+ 単語辞書を作成します。
+ 単語を単語番号に変換してシーケンスを作成します。

変換されたシーケンスを最大長にパディングし、入力シーケンスが完成します。
フランス語のデータセットには190,000の文ペアがあるので、適切な場所でカットし、順序をシャッフルします。

これらのことを実行するコードを以下に示します。
```php
ここにコードが入る
```


エンコーダ / デコーダモデル
-----------------------
"シーケンス・トゥ・シーケンス学習"でよく使用されるモデルには、エンコーダ / デコーダモデルがあります。

エンコーダで入力データから意味ベクトルを抽出し、
そのベクトルからデコーダを通過する際にターゲットデータを生成するように学習します。

ここでは、エンコーダとデコーダにGRUレイヤーを使用します。
GRUはリカレントニューラルネットワーク（RNN）の一つで、最近よく使用されています。
RNNを使用して次々と単語を予測します。

また、アテンションレイヤーも使用します。
アテンションは、入力単語に対応する出力単語に焦点を当てる効果があります。

エンコーダ
------------
入力シーケンスは、単語埋め込みベクトルをクエリするために埋め込みレイヤーを通過します。

GRUを通過したシーケンスの出力は、アテンション入力としてデコーダに渡されます。
入力シーケンスをベクトル化した結果としてのGRUステータスの出力を
デコーダのステータス入力に渡します。

デコーダ
-------
デコーダは少し複雑です。
ステータス入力が与えられると、対応するシーケンスを生成する必要があります。
<start>が与えられると次の単語が生成され、生成された単語が与えられると、デコーダは次の単語を生成するように訓練されます。
図に示すように出力すべき単語を入力する代わりに、最初から正しい単語のシーケンスを与えて学習することで効率が向上します。
入力と出力が1単語ずれていることに注意してください。

また、アテンションも使用します。
入力シーケンスの特定の単語が特定の出力シーケンスの単語に反応するように訓練されます。
特定の単語が出現したときに出力がより反応するようにします。
入力単語と出力単語の関連性の深さは、アテンション内でアテンションウェイトとして計算されます。
これを取り出して視覚化することができます。

損失関数とSeq2Seqモデル
--------------------------------
これまでに作成したデコーダとエンコーダを組み合わせて、目的のモデルを作成しましょう。

損失関数にはスパースカテゴリカルクロスエントロピーを使用します。
ただし、出力シーケンスを比較する際に、1単語ずれていることを覚えておいてください。
カスタムモデルの特別な方法を使用して、比較するためにシーケンスをずらしましょう。

さらに、訓練されたモデルに翻訳を実行させるメソッドを追加します。
訓練中に正しい出力シーケンス全体を与える代わりに、<start>から単語ごとに推論するだけです。
また、アテンションウェイトを視覚化します。

訓練
--------
モデルクラスが完成したので、データを与えて訓練します。

マシンに応じてテストできるスケールに合わせてパラメータを選択します。
主な条件は以下の通りです。

シーケンスデータを最初に作成しましょう。

モデルをインスタンス化してコンパイルします。

Seq2Seqモデルを訓練します。

予測
------
学習済みモデルで機械翻訳を行いましょう。

また、翻訳時にアテンションスコアを視覚化して、入力単語が特定の出力単語に反応するかどうかを確認します。

実験としてはシンプルなモデルですが、多くの場合、うまく翻訳されていることがわかります。

アテンションスコアの画像がうまく視覚化される場合とされない場合があるようです。


